# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.
"""

# !pip install numpy scikit-learn scikit-image matplotlib tensorflow keras visualkeras pandas pydot graphviz

from tp1_utils import load_data, images_to_pic, compare_masks, overlay_masks
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential
from keras.layers import MaxPooling2D, Conv2D, Flatten, Dense, Activation, Dropout
from keras import layers, models, backend
from keras import regularizers
from keras.optimizers import Adam, Adagrad, Adadelta
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
import visualkeras
import matplotlib.pyplot as plt
import numpy as np


assert callable(load_data) is True

ds = load_data()

for k, v in ds.items():
    print(k, v.shape)

images_to_pic("tp1_sample.png", ds["train_X"][:100])
images_to_pic("tp1_sample_masks.png", ds["train_masks"][:100])

"""# Image visualisation utilities"""

# Image Visualisation
train_X_vis = ds["train_X"]

plt.figure(figsize=(14, 10))
for i in range(20):
    rand_num = np.random.randint(0, train_X_vis.shape[0])
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    cifar_img = plt.subplot(4, 5, i + 1)
    plt.imshow(train_X_vis[rand_num])

print(ds["train_classes"][5])
print(ds["train_labels"][8])


def training_plot(history):
    acc = history.history["accuracy"]
    val_acc = history.history["val_accuracy"]
    loss = history.history["loss"]
    val_loss = history.history["val_loss"]
    epochs = range(1, len(acc) + 1)
    plt.plot(epochs, acc, "bo", label="Training acc")
    plt.plot(epochs, val_acc, "b", label="Validation acc")
    plt.title("Training and validation accuracy")
    plt.legend()
    plt.figure()
    plt.plot(epochs, loss, "bo", label="Training loss")
    plt.plot(epochs, val_loss, "b", label="Validation loss")
    plt.title("Training and validation loss")
    plt.legend()
    plt.show()


def plot_history_new(history):
    """
    Plots the training and validation accuracy and loss over epochs for a given model history.

    Args:
        history (keras.callbacks.History): The history object returned by model.fit().
    """
    acc = history.history["accuracy"]
    val_acc = history.history["val_accuracy"]
    loss = history.history["loss"]
    val_loss = history.history["val_loss"]
    epochs = range(1, len(acc) + 1)

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))

    ax1.plot(epochs, acc, "bo", label="Training accuracy")
    ax1.plot(epochs, val_acc, "b", label="Validation accuracy")
    ax1.set_title("Training and validation accuracy")
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Accuracy")
    ax1.legend()

    ax2.plot(epochs, loss, "bo", label="Training loss")
    ax2.plot(epochs, val_loss, "b", label="Validation loss")
    ax2.set_title("Training and validation loss")
    ax2.set_xlabel("Epoch")
    ax2.set_ylabel("Loss")
    ax2.legend()

    plt.tight_layout()
    plt.show()


backend.clear_session()

"""# 1. Multilayer Perceptron"""

backend.clear_session()

model_mlp = models.Sequential()
model_mlp.add(layers.Flatten(input_shape=(64, 64, 3)))

model_mlp.add(
    layers.Dense(
        32, activation="relu", kernel_initializer=tf.keras.initializers.HeNormal()
    )
)
model_mlp.add(layers.BatchNormalization())
model_mlp.add(layers.Dropout(0.21))

model_mlp.add(
    layers.Dense(
        64, activation="relu", kernel_initializer=tf.keras.initializers.HeNormal()
    )
)
model_mlp.add(layers.BatchNormalization())
model_mlp.add(layers.Dropout(0.21))

model_mlp.add(
    layers.Dense(
        64, activation="relu", kernel_regularizer=tf.keras.regularizers.L2(0.07)
    )
)
model_mlp.add(layers.Dense(10))


model_mlp.compile(
    optimizer=Adam(learning_rate=0.005),
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_accuracy",
    patience=10,
    verbose=1,
    mode="max",
    restore_best_weights=True,
)
redude_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_accuracy", factor=0.2, patience=4, verbose=1, mode="max", min_lr=0.0001
)
tenfor_board = tf.keras.callbacks.TensorBoard(
    log_dir="./logsMLP",
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    write_steps_per_second=False,
)

history_mlp = model_mlp.fit(
    ds["train_X"],
    ds["train_classes"],
    validation_split=0.125,
    epochs=40,
    batch_size=16,
    callbacks=[early_stop, redude_lr, tenfor_board],
)

loss, accuracy = model_mlp.evaluate(ds["test_X"], ds["test_classes"])
print("Test accuracy:", accuracy)

plot_history_new(history_mlp)
# dot_img_file = './model_MLP2.png'
# tf.keras.utils.plot_model(model_MLP, to_file=dot_img_file, show_shapes=True,show_layer_activations=True)
visualkeras.layered_view(model_mlp, legend=True)

"""# 2. Convolutional neural network

"""

model_cnn = models.Sequential()

model_cnn.add(
    layers.Conv2D(
        filters=64,
        kernel_size=(3, 3),
        activation="relu",
        padding="same",
        kernel_initializer=tf.keras.initializers.HeNormal(),
        input_shape=(64, 64, 3),
    )
)
model_cnn.add(layers.MaxPooling2D(pool_size=(3, 3), padding="same"))
model_cnn.add(layers.BatchNormalization())

model_cnn.add(
    layers.Conv2D(
        filters=32,
        kernel_size=(3, 3),
        activation="relu",
        padding="same",
        kernel_initializer=tf.keras.initializers.HeNormal(),
        kernel_regularizer=tf.keras.regularizers.L2(0.01),
    )
)
model_cnn.add(layers.MaxPooling2D(pool_size=(3, 3), padding="same"))
model_cnn.add(layers.BatchNormalization())

model_cnn.add(
    layers.Conv2D(
        filters=64,
        kernel_size=(3, 3),
        activation="relu",
        padding="same",
        kernel_initializer=tf.keras.initializers.HeNormal(),
    )
)
model_cnn.add(layers.MaxPooling2D(pool_size=(3, 3), padding="same"))
model_cnn.add(layers.BatchNormalization())

model_cnn.add(layers.Flatten())
model_cnn.add(
    layers.Dense(
        64, activation="relu", kernel_regularizer=tf.keras.regularizers.L2(0.01)
    )
)
model_cnn.add(layers.BatchNormalization())
model_cnn.add(layers.Dropout(0.15))

model_cnn.add(
    layers.Dense(
        32, activation="relu", kernel_regularizer=tf.keras.regularizers.L2(0.01)
    )
)
model_cnn.add(layers.BatchNormalization())
model_cnn.add(layers.Dropout(0.15))
model_cnn.add(layers.Dense(10))

model_cnn.compile(
    optimizer=Adam(learning_rate=0.005),
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_accuracy", patience=7, verbose=1, mode="max", restore_best_weights=True
)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_accuracy", factor=0.2, patience=3, verbose=1, mode="max", min_lr=0.0001
)

history_cnn = model_cnn.fit(
    ds["train_X"],
    ds["train_classes"],
    validation_split=0.125,
    epochs=25,
    batch_size=16,
    callbacks=[early_stop, reduce_lr],
)
plot_history_new(history_cnn)

loss, accuracy = model_cnn.evaluate(ds["test_X"], ds["test_classes"])
print("Test accuracy:", accuracy)

dot_img_file = "./model_CNN2.png"
tf.keras.utils.plot_model(
    model_cnn, to_file=dot_img_file, show_shapes=True, show_layer_activations=True
)
visualkeras.layered_view(model_cnn, legend=True)


def plot_predictions(model, X_test, y_test):
    y_pred = model.predict(X_test)

    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test_classes = np.argmax(y_test, axis=1)

    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(14, 14))
    for i, ax in enumerate(axes.flat):
        idx = np.random.randint(len(X_test))

        ax.imshow(X_test[idx])
        ax.set_title(f"Predicted: {y_pred_classes[idx]}, Actual: {y_test_classes[idx]}")
        ax.axis("off")

    plt.show()


plot_predictions(model_cnn, ds["test_X"], ds["test_classes"])

model_multilabel = models.Sequential()

model_multilabel.add(
    layers.Conv2D(
        filters=128,
        kernel_size=(3, 3),
        activation="relu",
        padding="same",
        kernel_initializer=tf.keras.initializers.HeNormal(),
        input_shape=(64, 64, 3),
    )
)
model_multilabel.add(layers.MaxPooling2D(pool_size=(3, 3), padding="same"))
model_multilabel.add(layers.BatchNormalization())

model_multilabel.add(
    layers.Conv2D(
        filters=128,
        kernel_size=(3, 3),
        activation="relu",
        padding="same",
        kernel_initializer=tf.keras.initializers.HeNormal(),
        kernel_regularizer=tf.keras.regularizers.L2(0.009),
    )
)
model_multilabel.add(layers.MaxPooling2D(pool_size=(3, 3), padding="same"))
model_multilabel.add(layers.BatchNormalization())
model_multilabel.add(layers.Dropout(0.2))

model_multilabel.add(
    layers.Conv2D(
        filters=64,
        kernel_size=(3, 3),
        activation="relu",
        padding="same",
        kernel_initializer=tf.keras.initializers.HeNormal(),
    )
)
model_multilabel.add(layers.MaxPooling2D(pool_size=(3, 3), padding="same"))
model_multilabel.add(layers.BatchNormalization())

model_multilabel.add(layers.Flatten())
model_multilabel.add(
    layers.Dense(
        64, activation="relu", kernel_regularizer=tf.keras.regularizers.L2(0.008)
    )
)
model_multilabel.add(layers.BatchNormalization())
model_multilabel.add(layers.Dropout(0.2))

model_multilabel.add(
    layers.Dense(
        32, activation="relu", kernel_regularizer=tf.keras.regularizers.L2(0.007)
    )
)
model_multilabel.add(layers.BatchNormalization())
model_multilabel.add(layers.Dropout(0.2))
model_multilabel.add(layers.Dense(10, activation="sigmoid"))

model_multilabel.compile(
    optimizer=Adam(learning_rate=0.003),
    loss="binary_crossentropy",
    metrics=["accuracy"],
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_accuracy",
    patience=10,
    verbose=1,
    mode="max",
    restore_best_weights=True,
)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_accuracy", factor=0.2, patience=4, verbose=1, mode="max", min_lr=0.0001
)

history_multilabel = model_multilabel.fit(
    ds["train_X"],
    ds["train_labels"],
    validation_split=0.125,
    epochs=40,
    batch_size=16,
    callbacks=[early_stop, reduce_lr],
)
training_plot(history_multilabel)

"""# 4. Transfer learning
## Common cells
"""

from tensorflow.keras.applications.mobilenet import MobileNet
from tensorflow.keras.applications.mobilenet import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_dataset = tf.image.resize(ds["train_X"], size=(224, 224))
test_dataset = tf.image.resize(ds["test_X"], size=(224, 224))

print(np.shape(train_dataset))

data_augmentation = tf.keras.Sequential(
    [
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.2),
        tf.keras.layers.RandomZoom(0.005),
    ]
)

input_shape = 224, 224, 3
feature_extractor = MobileNet(
    input_shape=input_shape, weights="imagenet", include_top=False
)
feature_extractor.trainable = False


def create_classificator(feature_extractor, last_layer_activation="sigmoid"):
    classificator = tf.keras.models.Sequential()
    classificator.add(feature_extractor)
    classificator.add(tf.keras.layers.GlobalAveragePooling2D())
    classificator.add(
        tf.keras.layers.Dense(256, activation="relu", kernel_initializer="he_normal")
    )
    classificator.add(tf.keras.layers.BatchNormalization())
    classificator.add(tf.keras.layers.Dropout(0.1))
    classificator.add(
        tf.keras.layers.Dense(128, activation="relu", kernel_initializer="he_normal")
    )
    classificator.add(tf.keras.layers.Dropout(0.1))
    classificator.add(
        tf.keras.layers.Dense(128, activation="relu", kernel_initializer="he_normal")
    )
    classificator.add(tf.keras.layers.Dropout(0.1))
    classificator.add(
        tf.keras.layers.Dense(64, activation="relu", kernel_initializer="he_normal")
    )
    classificator.add(
        Dense(10, activation=last_layer_activation, kernel_initializer="he_normal")
    )

    return classificator


"""## Multiclass single label"""

model = create_classificator(feature_extractor)
learning_rate = 0.01
model.compile(
    optimizer=Adam(learning_rate=learning_rate),
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_accuracy",
    patience=10,
    verbose=1,
    mode="max",
    restore_best_weights=True,
)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_accuracy", factor=0.2, patience=4, verbose=1, mode="max", min_lr=0.0001
)
history = model.fit(
    train_dataset,
    ds["train_classes"],
    validation_split=0.125,
    epochs=60,
    callbacks=[early_stop, reduce_lr],
)

plot_history_new(history)
loss, accuracy = model.evaluate(test_dataset, ds["test_classes"])
print("Test accuracy:", accuracy)

model.summary()

"""## Fine-tuning"""

model.layers[0].trainable = True
for layer in model.layers[0].layers[:-10]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate / 10),
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)

model.summary()

history = model.fit(
    train_dataset,
    ds["train_classes"],
    validation_split=0.125,
    epochs=60 + 20,
    callbacks=[early_stop, reduce_lr],
    initial_epoch=history.epoch[-1],
)

plot_history_new(history)
loss, accuracy = model.evaluate(test_dataset, ds["test_classes"])
print("Test accuracy:", accuracy)

"""## Multiclass multilabel

"""

model = create_classificator(feature_extractor)
learning_rate = 0.01
model.compile(
    optimizer=Adam(learning_rate=learning_rate),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=["accuracy", tf.keras.metrics.Recall()],
)

model.summary()

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_accuracy",
    patience=10,
    verbose=1,
    mode="max",
    restore_best_weights=True,
)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_accuracy", factor=0.2, patience=4, verbose=1, mode="max", min_lr=0.0001
)

history = model.fit(
    train_dataset,
    ds["train_labels"],
    validation_split=0.125,
    epochs=60,
    callbacks=[early_stop, reduce_lr],
)

plot_history_new(history)
loss, accuracy, recall = model.evaluate(test_dataset, ds["test_labels"])
print("Test accuracy:", accuracy)
print("Test recall:", recall)

"""## Fine-tuning"""

model.layers[0].trainable = True
for layer in model.layers[0].layers[:-10]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate / 10),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=["accuracy", tf.keras.metrics.Recall()],
)

model.summary()

history = model.fit(
    train_dataset,
    ds["train_labels"],
    validation_split=0.125,
    epochs=60 + 20,
    callbacks=[early_stop, reduce_lr],
    initial_epoch=history.epoch[-1],
)

plot_history_new(history)
loss, accuracy, recall = model.evaluate(test_dataset, ds["test_labels"])
print("Test accuracy:", accuracy)
print("Test recall:", recall)

"""# Convolution Activaton Mapping"""


def visualize_activations(model, images):
    img = images[np.random.randint(images.shape[0]), :, :, :]

    layer_outputs = [layer.output for layer in model.layers if "conv" in layer.name]
    activation_model = tf.keras.models.Model(inputs=model.inputs, outputs=layer_outputs)
    activations = activation_model.predict(np.expand_dims(img, axis=0))

    fig, axs = plt.subplots(nrows=1, ncols=len(activations))
    fig.set_size_inches(15, 5)

    for i, activation in enumerate(activations):
        axs[i].imshow(activation[0, :, :, 0], cmap="viridis")
        axs[i].axis("off")

    plt.show()


def visualize_all_activations(model, images):
    img = images[np.random.randint(images.shape[0]), :, :, :]

    layer_outputs = [layer.output for layer in model.layers if "conv" in layer.name]
    activation_model = tf.keras.models.Model(inputs=model.inputs, outputs=layer_outputs)
    activations = activation_model.predict(np.expand_dims(img, axis=0))

    fig, axs = plt.subplots(nrows=1, ncols=len(activations) + 1)
    fig.set_size_inches(20, 5)

    axs[0].imshow(img)
    axs[0].axis("off")
    axs[0].set_title("Original Image")

    for i, activation in enumerate(activations):
        axs[i + 1].imshow(activation[0, :, :, 0], cmap="viridis")
        axs[i + 1].axis("off")
        axs[i + 1].set_title(f"Convolutional Layer {i+1}")

    plt.show()


visualize_all_activations(model_cnn, ds["test_X"])
